{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Niranjan_Work\\dnns_qualitative\\dnn_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vgg16 with 54 layers\n",
      "Computing layerwise activations for singleton images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:07<00:00, 18.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying visually active neurons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:40<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing layerwise activations for pair and triplet images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:03<00:00, 15.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'D:/Niranjan_Work/dnns_qualitative/dnn_perception_expts')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "from _utils.data import load_multiple_stim_files, load_stim_file\n",
    "from _utils.network import load_model, get_layerwise_activations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "stim_data = load_stim_file('./data/div_norm_stim.mat')\n",
    "# print(stim_data.shape) # images: (267, 3, 224, 224)\n",
    "# 49 objects\n",
    "# singleton => in top, mid or bottom => 3x49 = 147\n",
    "# pair => in 2 of the 3 positions => 60\n",
    "# triplet => in 3 of the 3 positions => 60\n",
    "# total => 267\n",
    "\n",
    "stim_pos =  sio.loadmat('./data/div_norm_stim_pos.mat', squeeze_me=True, struct_as_record=True)['stim_pos']\n",
    "# print(stim_pos.shape) # image_positions: (267, 3)\n",
    "# for each img in stim_data, positions encoded in 3d vector e.g. [1, 2, 999] means top=obj1, mid=obj2, bottom=blank\n",
    "\n",
    "n_stim = 267\n",
    "n_images = 147\n",
    "singleton_ind = range(0, n_images)\n",
    "pair_ind = range(n_images, n_images + 60)\n",
    "pair_i_start = 147\n",
    "triplet_ind = range(n_images + 60, n_images + 120)\n",
    "triplet_i_start = 207\n",
    "var_threshold = 0.1\n",
    "\n",
    "layers = load_model('vgg16')\n",
    "n_layers = len(layers)\n",
    "singleton_representations = []\n",
    "\n",
    "print(\"Computing layerwise activations for singleton images...\")\n",
    "for stim_i in tqdm(range(n_images)):\n",
    "    img_rep = get_layerwise_activations(stim_data[stim_i])\n",
    "    singleton_representations.append(img_rep)\n",
    "\n",
    "single_group = np.arange(0, n_images)\n",
    "single_group = np.reshape(single_group, (49,3))\n",
    "\n",
    "neurons_per_layer = {} #stores n_units per layer\n",
    "visually_active_neurons_per_layer = {} #\n",
    "\n",
    "# Identify Visually Active Neurons\n",
    "print(\"Identifying visually active neurons...\")\n",
    "for layer_i in tqdm(range(n_layers)):\n",
    "    layer_name = layers[layer_i]\n",
    "    n_units = len(singleton_representations[0][layer_name].flatten())\n",
    "    neurons_per_layer[layer_name] = n_units\n",
    "    layerwise_responses = np.zeros((n_images, n_units))\n",
    "\n",
    "    for stim_i in range(n_images):\n",
    "        img_rep = singleton_representations[stim_i][layer_name]\n",
    "        layerwise_responses[stim_i, :] = img_rep.flatten()\n",
    "\n",
    "    reps_top = layerwise_responses[single_group[:, 0], :]\n",
    "    reps_mid = layerwise_responses[single_group[:, 1], :]\n",
    "    reps_bot = layerwise_responses[single_group[:, 2], :]\n",
    "    \n",
    "    var_reps_top = np.var(reps_top, axis=0)\n",
    "    var_reps_mid = np.var(reps_mid, axis=0)\n",
    "    var_reps_bot = np.var(reps_bot, axis=0)\n",
    "    \n",
    "    van_indices = np.where((var_reps_top > var_threshold) & (var_reps_mid > var_threshold) & (var_reps_bot > var_threshold))[0]\n",
    "    visually_active_neurons_per_layer[layer_name] = van_indices\n",
    "\n",
    "pair_representations = []\n",
    "triplet_representations = []\n",
    "\n",
    "# Extract features for pairs, triplets\n",
    "print(\"Computing layerwise activations for pair and triplet images...\")\n",
    "for stim_i, stim in enumerate(tqdm(stim_data[pair_ind])):\n",
    "    img_rep = get_layerwise_activations(stim)\n",
    "    pair_representations.append(img_rep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for stim_i, stim in enumerate(tqdm(stim_data[triplet_ind])):\n",
    "#     img_rep = get_layerwise_activations(stim)\n",
    "#     triplet_representations.append(img_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "\n",
    "def find_normalization_combined(Fcombined, Fsum):\n",
    "    fc_max = np.max(Fcombined)\n",
    "    fsum_max = np.max(Fsum)\n",
    "    C = max(fc_max, fsum_max)\n",
    "    \n",
    "    # Prepare the data for regression\n",
    "    X = np.column_stack([Fsum.flatten()/C, np.ones(len(Fsum.flatten()))])\n",
    "    y = Fcombined.flatten() /C\n",
    "    \n",
    "    # Perform linear regression\n",
    "    model = LinearRegression(fit_intercept=False)  # No intercept because it's included in X\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Get regression coefficients\n",
    "    coeff_combined = model.coef_\n",
    "    \n",
    "    return coeff_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_layers = []\n",
    "active_layer_inds = []\n",
    "for layer_i, layer in enumerate(layers):\n",
    "    if len(visually_active_neurons_per_layer[layer]) > 1:\n",
    "        active_layers.append(layer)\n",
    "        active_layer_inds.append(layer_i)\n",
    "\n",
    "model_layers = [active_layers, active_layer_inds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_layer_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing normalization slopes for pairs\n",
    "pair_stim_slopes = np.ones((60,len(active_layers)))\n",
    "for stim_i in tqdm(range(60)):\n",
    "    individual_representations = []\n",
    "    combined_representations = pair_representations[stim_i] \n",
    "    for pos_i in range(3):\n",
    "        # print(stim_pos[pair_i_start + stim_i, pos_i])\n",
    "        if stim_pos[pair_i_start + stim_i, pos_i] != 999:\n",
    "                singleton_stim_ind = np.where(stim_pos[singleton_ind][:, pos_i] == stim_pos[pair_ind][stim_i, pos_i])[0][0]\n",
    "                individual_representations.append(singleton_representations[singleton_stim_ind])\n",
    "    for layer_i, layer in enumerate(active_layers):\n",
    "         van_indices = visually_active_neurons_per_layer[layer]\n",
    "         f_single = individual_representations[0][layer].flatten()[van_indices] + individual_representations[1][layer].flatten()[van_indices]\n",
    "         f_pair = combined_representations[layer].flatten()[van_indices]\n",
    "         coeff = find_normalization_combined(f_pair.numpy(), f_single.numpy())\n",
    "         pair_stim_slopes[stim_i, layer_i] = coeff[0]\n",
    "\n",
    "layerwise_pair_slopes = np.mean(pair_stim_slopes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing layerwise activations for triplet images...\")\n",
    "for stim_i, stim in enumerate(tqdm(stim_data[triplet_ind])):\n",
    "    img_rep = get_layerwise_activations(stim)\n",
    "    triplet_representations.append(img_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing normalization slopes for triplet images...\")\n",
    "# computing normalization slopes\n",
    "triplet_stim_slopes = np.ones((60,len(active_layers)))\n",
    "for stim_i in tqdm(range(60)):\n",
    "    individual_representations = []\n",
    "    combined_representations = triplet_representations[stim_i] \n",
    "    for pos_i in range(3):\n",
    "        # print(stim_pos[pair_i_start + stim_i, pos_i])\n",
    "        if stim_pos[triplet_i_start + stim_i, pos_i] != 999:\n",
    "                singleton_stim_ind = np.where(stim_pos[singleton_ind][:, pos_i] == stim_pos[triplet_ind][stim_i, pos_i])[0][0]\n",
    "                individual_representations.append(singleton_representations[singleton_stim_ind])\n",
    "\n",
    "    for layer_i, layer in enumerate(active_layers):\n",
    "        van_indices = visually_active_neurons_per_layer[layer]\n",
    "        f_single = individual_representations[0][layer].flatten()[van_indices] + individual_representations[1][layer].flatten()[van_indices] + individual_representations[2][layer].flatten()[van_indices]\n",
    "        f_combined = combined_representations[layer].flatten()[van_indices]\n",
    "        coeff = find_normalization_combined(f_combined.numpy(), f_single.numpy())\n",
    "        triplet_stim_slopes[stim_i, layer_i] = coeff[0]\n",
    "\n",
    "layerwise_triplet_slopes = np.mean(triplet_stim_slopes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerwise_pair_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerwise_triplet_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
